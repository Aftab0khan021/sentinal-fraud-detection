# TorchServe Configuration for SentinAL Fraud Detection Model

# Inference address (HTTP)
inference_address=http://0.0.0.0:8080

# Management address (for model management APIs)
management_address=http://0.0.0.0:8081

# Metrics address (Prometheus metrics)
metrics_address=http://0.0.0.0:8082

# Model store location
model_store=/home/model-server/model-store

# Load models on startup
load_models=fraud_detection.mar

# Number of workers per model (adjust based on CPU cores)
# For GPU: set to number of GPUs
# For CPU: set to number of CPU cores / 2
default_workers_per_model=2

# Response timeout (seconds)
default_response_timeout=120

# Maximum request size (MB)
max_request_size=10

# Maximum response size (MB)
max_response_size=10

# Job queue size
job_queue_size=100

# Number of netty threads (for handling requests)
number_of_netty_threads=8

# Enable metrics
enable_metrics_api=true

# Metrics format (prometheus or legacy)
metrics_format=prometheus

# CORS settings
cors_allowed_origin=*
cors_allowed_methods=GET,POST,PUT,DELETE,OPTIONS
cors_allowed_headers=*

# Logging
# Options: DEBUG, INFO, WARN, ERROR
default_log_level=INFO

# Model snapshot (save model state)
snapshot={
  "name": "startup.cfg",
  "modelCount": 1,
  "models": {
    "fraud_detection": {
      "1.0": {
        "defaultVersion": true,
        "marName": "fraud_detection.mar",
        "minWorkers": 1,
        "maxWorkers": 4,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}

# GPU configuration (uncomment if using GPU)
# number_of_gpu=1
# gpu_id=0

# Batch inference settings
# Enable batching for better throughput
enable_envvars_config=true
batch_size=8
max_batch_delay=100

# Async logging
async_logging=true

# Prefer direct buffer for better performance
prefer_direct_buffer=true

# Disable model versioning for simplicity (can enable in production)
model_snapshot_disabled=false

# Workflow configuration
workflow_store=/home/model-server/workflow-store
